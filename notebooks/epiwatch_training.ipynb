{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4f8cd5",
   "metadata": {},
   "source": [
    "# EpiWatch: AI for Early Epidemic Detection\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Mission:** Create a scalable and intelligent system that detects early signs of disease outbreaks in low-resource regions.\n",
    "\n",
    "**UN SDG Alignment:**\n",
    "- **SDG 3**: Good Health & Well-being\n",
    "- **SDG 9**: Industry, Innovation & Infrastructure\n",
    "- **SDG 10**: Reduced Inequalities\n",
    "\n",
    "## Notebook Goals\n",
    "1. Train 5 models (1 custom + 4 pre-trained)\n",
    "2. Compare performance metrics\n",
    "3. Select best model for mobile app\n",
    "4. Generate outputs for app integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d64906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ab85f",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.text_preprocessing import DatasetBuilder, TextPreprocessor\n",
    "\n",
    "# Initialize\n",
    "builder = DatasetBuilder()\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Create or load dataset\n",
    "data_path = '../data/processed/epidemic_data.csv'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Creating sample dataset...\")\n",
    "    df = builder.create_sample_dataset(n_samples=2000, save_path=data_path)\n",
    "else:\n",
    "    print(f\"Loading dataset from {data_path}...\")\n",
    "    df = builder.load_data(data_path)\n",
    "\n",
    "print(f\"\\n‚úì Dataset loaded: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd993b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nRegion distribution:\")\n",
    "print(df['region'].value_counts())\n",
    "print(f\"\\nDisease distribution:\")\n",
    "print(df['disease'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49992e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Class distribution\n",
    "df['label'].value_counts().plot(kind='bar', ax=axes[0], color=['#4CAF50', '#FF4444'])\n",
    "axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Label (0=Normal, 1=Outbreak)')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Region distribution\n",
    "df['region'].value_counts().plot(kind='bar', ax=axes[1], color='#3498db')\n",
    "axes[1].set_title('Region Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Region')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# Disease distribution\n",
    "df['disease'].value_counts().plot(kind='bar', ax=axes[2], color='#e74c3c')\n",
    "axes[2].set_title('Disease Distribution', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Disease')\n",
    "axes[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3904a6a",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text preprocessing\n",
    "example_text = df['text'].iloc[0]\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(\"=\" * 60)\n",
    "print(example_text)\n",
    "print()\n",
    "\n",
    "print(\"Cleaned Text:\")\n",
    "print(\"=\" * 60)\n",
    "cleaned = preprocessor.clean_text(example_text)\n",
    "print(cleaned)\n",
    "print()\n",
    "\n",
    "print(\"Fully Preprocessed Text:\")\n",
    "print(\"=\" * 60)\n",
    "processed = preprocessor.preprocess(example_text)\n",
    "print(processed)\n",
    "print()\n",
    "\n",
    "print(\"Extracted Features:\")\n",
    "print(\"=\" * 60)\n",
    "features = preprocessor.extract_features(example_text)\n",
    "for key, value in features.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all texts\n",
    "print(\"Preprocessing all texts...\")\n",
    "df['processed_text'] = preprocessor.preprocess_dataset(df['text'].tolist(), show_progress=True)\n",
    "print(\"\\n‚úì Preprocessing complete!\")\n",
    "\n",
    "# Show comparison\n",
    "comparison_df = df[['text', 'processed_text', 'label']].head(5)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9f9b5",
   "metadata": {},
   "source": [
    "## 3. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7117e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance dataset\n",
    "df_balanced = builder.balance_dataset(df, text_col='processed_text')\n",
    "\n",
    "# Split into train, val, test\n",
    "data_splits = builder.prepare_train_test_split(\n",
    "    df_balanced,\n",
    "    text_col='processed_text',\n",
    "    test_size=0.2,\n",
    "    val_size=0.1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Data split complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18ff5a",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "We will train 5 models:\n",
    "1. **Custom Neural Network** (LSTM + Attention) - Built from scratch\n",
    "2. **XLM-RoBERTa** - Cross-lingual pre-trained model\n",
    "3. **mBERT** - Multilingual BERT\n",
    "4. **DistilBERT** - Efficient distilled model\n",
    "5. **MuRIL** - Multilingual Representations for Indian Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1168671",
   "metadata": {},
   "source": [
    "### 4.1 Custom Neural Network (From Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from models.custom_model import CustomEpiDetector, ModelTrainer, build_vocab, EpidemicDataset\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"Building vocabulary...\")\n",
    "vocab = build_vocab(data_splits['train']['texts'], min_freq=2)\n",
    "print(f\"‚úì Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EpidemicDataset(data_splits['train']['texts'], data_splits['train']['labels'], vocab)\n",
    "val_dataset = EpidemicDataset(data_splits['val']['texts'], data_splits['val']['labels'], vocab)\n",
    "test_dataset = EpidemicDataset(data_splits['test']['texts'], data_splits['test']['labels'], vocab)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "print(f\"‚úì DataLoaders created\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize custom model\n",
    "custom_model = CustomEpiDetector(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=256,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(\"Custom Neural Network Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "print(custom_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in custom_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a074017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train custom model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "trainer = ModelTrainer(custom_model, device=device)\n",
    "train_losses, val_losses = trainer.train(train_loader, val_loader, epochs=5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Val Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(trainer.train_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.plot(trainer.val_accuracies, label='Val Accuracy', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7404c",
   "metadata": {},
   "source": [
    "### 4.2 Pre-trained Transformer Models\n",
    "\n",
    "**Note:** Training transformer models can be time-consuming. For this notebook, we'll demonstrate with one model. Use the `train_all.py` script to train all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d3487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pretrained_models import PretrainedEpiDetector\n",
    "\n",
    "# Train one pre-trained model as example (DistilBERT - fastest)\n",
    "print(\"Training DistilBERT (Fastest pre-trained model)...\")\n",
    "\n",
    "distilbert_model = PretrainedEpiDetector('distilbert-base-multilingual-cased', device=device)\n",
    "\n",
    "# Prepare dataloaders\n",
    "train_loader_bert = distilbert_model.prepare_dataloader(\n",
    "    data_splits['train']['texts'],\n",
    "    data_splits['train']['labels'],\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "val_loader_bert = distilbert_model.prepare_dataloader(\n",
    "    data_splits['val']['texts'],\n",
    "    data_splits['val']['labels'],\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"‚úì DataLoaders prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DistilBERT\n",
    "distilbert_model.train(train_loader_bert, val_loader_bert, epochs=3, learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfbd862",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.model_evaluator import ModelEvaluator\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Evaluate custom model\n",
    "print(\"Evaluating Custom Neural Network...\")\n",
    "predictions_custom, _ = trainer.predict(test_loader)\n",
    "y_pred_custom = (predictions_custom > 0.5).astype(int)\n",
    "y_true = np.array(data_splits['test']['labels'])\n",
    "\n",
    "evaluator.evaluate_model(\n",
    "    \"Custom Neural Network\",\n",
    "    y_true,\n",
    "    y_pred_custom,\n",
    "    y_prob=predictions_custom,\n",
    "    inference_time=0.05,\n",
    "    model_size=50\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Custom model evaluated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77730fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate DistilBERT\n",
    "print(\"Evaluating DistilBERT...\")\n",
    "predictions_distil, probabilities_distil = distilbert_model.predict(\n",
    "    data_splits['test']['texts'],\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Measure inference time\n",
    "sample_text = data_splits['test']['texts'][0]\n",
    "timing = distilbert_model.measure_inference_time(sample_text, num_runs=50)\n",
    "\n",
    "evaluator.evaluate_model(\n",
    "    \"DistilBERT-Multilingual\",\n",
    "    y_true,\n",
    "    predictions_distil,\n",
    "    y_prob=probabilities_distil,\n",
    "    inference_time=timing['mean'],\n",
    "    model_size=270\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì DistilBERT evaluated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison table\n",
    "comparison_df = evaluator.get_comparison_table()\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations\n",
    "evaluator.plot_comparison(save_path='../outputs/visualizations/model_comparison.png')\n",
    "evaluator.plot_confusion_matrices(save_path='../outputs/visualizations/confusion_matrices.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendation\n",
    "recommendation = evaluator.generate_recommendation()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üèÜ RECOMMENDED MODEL FOR EPIWATCH APPLICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nModel: {recommendation['recommended_model']}\")\n",
    "print(f\"Overall Score: {recommendation['total_score']:.4f}\")\n",
    "print(f\"\\nReasons:\")\n",
    "for reason in recommendation['reasons']:\n",
    "    print(f\"  ‚úì {reason}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20037d69",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection and Alert Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c928dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.anomaly_detection import AnomalyDetector, OutbreakAlertSystem\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create sample predictions with temporal and spatial data\n",
    "dates = pd.date_range(start='2024-11-01', periods=100, freq='H')\n",
    "regions = ['Mumbai', 'Delhi', 'Bangalore', 'Chennai']\n",
    "diseases = ['Dengue', 'COVID-19', 'Malaria', 'Influenza']\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'date': np.random.choice(dates, 500),\n",
    "    'region': np.random.choice(regions, 500),\n",
    "    'disease': np.random.choice(diseases, 500),\n",
    "    'text': ['sample text'] * 500,\n",
    "    'prediction': np.random.binomial(1, 0.3, 500),\n",
    "    'probability': np.random.uniform(0.3, 0.95, 500)\n",
    "})\n",
    "\n",
    "# Add some anomalies (outbreaks)\n",
    "outbreak_indices = np.random.choice(500, 50, replace=False)\n",
    "predictions_df.loc[outbreak_indices, 'prediction'] = 1\n",
    "predictions_df.loc[outbreak_indices, 'probability'] = np.random.uniform(0.8, 0.99, 50)\n",
    "\n",
    "print(f\"‚úì Sample predictions created: {len(predictions_df)} records\")\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize alert system\n",
    "detector = AnomalyDetector(method='zscore', threshold=2.5)\n",
    "alert_system = OutbreakAlertSystem(detector)\n",
    "\n",
    "# Process and generate alerts\n",
    "alerts = alert_system.process_and_alert(predictions_df)\n",
    "\n",
    "print(f\"\\nüö® Generated {len(alerts)} alerts\\n\")\n",
    "\n",
    "# Display sample alerts\n",
    "for i, alert in enumerate(alerts[:5]):\n",
    "    print(f\"Alert {i+1}:\")\n",
    "    print(f\"  {alert['message']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6941b81d",
   "metadata": {},
   "source": [
    "## 7. Mobile App Integration - Output Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c40e3b",
   "metadata": {},
   "source": [
    "### 7.1 Alert Feed (for Recent Alerts Screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78716562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get alerts for mobile app\n",
    "mobile_alerts = alert_system.get_alerts_for_mobile(limit=10)\n",
    "\n",
    "print(\"üì± Mobile App - Recent Alerts\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for alert in mobile_alerts[:5]:\n",
    "    color_emoji = {'high': 'üî¥', 'moderate': 'üü†', 'low': 'üü¢'}\n",
    "    print(f\"\\n{color_emoji[alert['risk_level']]} {alert['title']}\")\n",
    "    print(f\"   Location: {alert['location']}\")\n",
    "    print(f\"   Cases: {alert['case_count']}\")\n",
    "    print(f\"   Risk: {alert['risk_level'].upper()}\")\n",
    "    print(f\"   Summary: {alert['summary'][:100]}...\")\n",
    "\n",
    "# Save to JSON for mobile app\n",
    "import json\n",
    "with open('../outputs/alerts/mobile_alerts.json', 'w') as f:\n",
    "    json.dump(mobile_alerts, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n‚úì Alerts saved for mobile app: outputs/alerts/mobile_alerts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc299ac9",
   "metadata": {},
   "source": [
    "### 7.2 Map Data (for Global Outbreak Status Screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7047cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get map data\n",
    "map_data = alert_system.get_map_data()\n",
    "\n",
    "print(\"üì± Mobile App - Map Data\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for region in map_data[:5]:\n",
    "    print(f\"\\n{region['region']}:\")\n",
    "    print(f\"  Max Risk Level: {region['max_risk']}\")\n",
    "    print(f\"  Active Alerts: {len(region['alerts'])}\")\n",
    "\n",
    "# Save to JSON\n",
    "with open('../outputs/visualizations/map_data.json', 'w') as f:\n",
    "    json.dump(map_data, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n‚úì Map data saved: outputs/visualizations/map_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592bec0",
   "metadata": {},
   "source": [
    "### 7.3 Trend Data (for 7-Day Disease Trends Screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930eb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 7-day trend data\n",
    "trend_data = alert_system.get_trend_data(days=7)\n",
    "\n",
    "print(\"üì± Mobile App - 7-Day Trend Data\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for disease_name, disease_data in list(trend_data.items())[:3]:\n",
    "    print(f\"\\n{disease_name}:\")\n",
    "    for day_data in disease_data['data'][-3:]:\n",
    "        print(f\"  {day_data['date']}: {day_data['count']} cases\")\n",
    "\n",
    "# Save to JSON\n",
    "with open('../outputs/visualizations/trend_data.json', 'w') as f:\n",
    "    json.dump(trend_data, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n‚úì Trend data saved: outputs/visualizations/trend_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9045630",
   "metadata": {},
   "source": [
    "### 7.4 Visualize Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 7-day trends\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('7-Day Disease Trends for Mobile App', fontsize=16, fontweight='bold')\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (disease_name, disease_data) in enumerate(list(trend_data.items())[:6]):\n",
    "    dates = [d['date'] for d in disease_data['data']]\n",
    "    counts = [d['count'] for d in disease_data['data']]\n",
    "    \n",
    "    axes[i].bar(range(len(dates)), counts, color='#3498db', alpha=0.7)\n",
    "    axes[i].set_title(disease_name, fontweight='bold')\n",
    "    axes[i].set_xlabel('Day')\n",
    "    axes[i].set_ylabel('Cases')\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add total\n",
    "    total = sum(counts)\n",
    "    axes[i].text(0.5, 0.95, f'Total: {total}', \n",
    "                transform=axes[i].transAxes,\n",
    "                ha='center', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/visualizations/7day_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Trend visualization saved: outputs/visualizations/7day_trends.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f3ef9b",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14068840",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã EPIWATCH PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ COMPLETED:\")\n",
    "print(\"  1. ‚úì Data loading and preprocessing\")\n",
    "print(\"  2. ‚úì Custom neural network training (from scratch)\")\n",
    "print(\"  3. ‚úì Pre-trained model fine-tuning (DistilBERT)\")\n",
    "print(\"  4. ‚úì Model evaluation and comparison\")\n",
    "print(\"  5. ‚úì Anomaly detection and alert generation\")\n",
    "print(\"  6. ‚úì Mobile app output generation\")\n",
    "\n",
    "print(\"\\nüì± MOBILE APP INTEGRATION:\")\n",
    "print(\"  ‚Ä¢ Alert Feed JSON: outputs/alerts/mobile_alerts.json\")\n",
    "print(\"  ‚Ä¢ Map Data JSON: outputs/visualizations/map_data.json\")\n",
    "print(\"  ‚Ä¢ Trend Data JSON: outputs/visualizations/trend_data.json\")\n",
    "print(\"  ‚Ä¢ Visualizations: outputs/visualizations/\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"  1. Train remaining pre-trained models (XLM-RoBERTa, mBERT, MuRIL)\")\n",
    "print(\"     ‚Üí Run: python src/models/train_all.py\")\n",
    "print(\"  2. Compare all 5 models and select the best\")\n",
    "print(\"  3. Deploy the best model with FastAPI\")\n",
    "print(\"     ‚Üí Run: uvicorn src.api.main:app --reload\")\n",
    "print(\"  4. Connect mobile app to API endpoints\")\n",
    "print(\"  5. Deploy to production (AWS/Azure/GCP)\")\n",
    "\n",
    "print(\"\\nüåç SDG IMPACT:\")\n",
    "print(\"  ‚Ä¢ SDG 3: Faster epidemic response ‚Üí Reduced mortality\")\n",
    "print(\"  ‚Ä¢ SDG 9: Innovative AI infrastructure for public health\")\n",
    "print(\"  ‚Ä¢ SDG 10: Focusing on low-resource regions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ú® EpiWatch: Saving lives through AI-powered early detection ‚ú®\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
